1.  CNN for Text-Based Multiple Choice Question Answering
    链接：https://aclanthology.coli.uni-saarland.de/papers/P18-2044/p18-2044
    Abstract： The task of Question Answering is at the very core of machine comprehension. In this paper, we propose a Convolutional Neural Network (CNN) model for textbased multiple choice question answering where questions are based on a particular article. Given an article and a multiple choice question, our model assigns a score to each question-option tuple and chooses the final option accordingly. We test our model on Textbook Question Answering (TQA) and SciQ dataset. Our model outperforms several LSTM-based baseline models on the two datasets.


2.  Active learning for deep semantic parsing
    链接：https://aclanthology.coli.uni-saarland.de/papers/P18-2008/p18-2008
    Abstract： Semantic parsing requires training data that is expensive and slow to collect. We apply active learning to both traditional and “overnight” data collection approaches. We show that it is possible to obtain good training hyperparameters from seed data which is only a small fraction of the full dataset. We show that uncertainty sampling based on least confidence score is competitive in traditional data collection but not applicable for overnight collection. We evaluate several active learning strategies for overnight data collection and show that different example selection strategies per domain perform best.
 


3.  GNEG: Graph-Based Negative Sampling for word2vec
    链接：https://aclanthology.coli.uni-saarland.de/papers/P18-2090/p18-2090
    Abstract: Negative sampling is an important component in word2vec for distributed word representation learning. We hypothesize that taking into account global, corpuslevel information and generating a different noise distribution for each target word better satisfies the requirements of negative examples for each training word than the original frequency-based distribution. In this purpose we pre-compute word cooccurrence statistics from the corpus and apply to it network algorithms such as random walk. We test this hypothesis through a set of experiments whose results show that our approach boosts the word analogy task by about 5% and improves the performance on word similarity tasks by about 1% compared to the skip-gram negative sampling baseline.
