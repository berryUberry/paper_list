1.  CNN for Text-Based Multiple Choice Question Answering
    链接：https://aclanthology.coli.uni-saarland.de/papers/P18-2044/p18-2044
    Abstract： The task of Question Answering is at the very core of machine comprehension. In this paper, we propose a Convolutional Neural Network (CNN) model for textbased multiple choice question answering where questions are based on a particular article. Given an article and a multiple choice question, our model assigns a score to each question-option tuple and chooses the final option accordingly. We test our model on Textbook Question Answering (TQA) and SciQ dataset. Our model outperforms several LSTM-based baseline models on the two datasets.

    Question Answering的任务是机器理解的核心。 在本文中，我们提出了基于文本的多项选择问答的卷积神经网络（CNN）模型，其中问题基于特定的文章。 给定一篇文章和一个多项选择题，我们的模型会为每个问题选项元组分配一个分数，并相应地选择最终选项。 我们在Textbook Question Answering（TQA）和SciQ数据集上测试我们的模型。 我们的模型在两个数据集上优于几个基于LSTM的基线模型。


2.  Active learning for deep semantic parsing
    链接：https://aclanthology.coli.uni-saarland.de/papers/P18-2008/p18-2008
    Abstract： Semantic parsing requires training data that is expensive and slow to collect. We apply active learning to both traditional and “overnight” data collection approaches. We show that it is possible to obtain good training hyperparameters from seed data which is only a small fraction of the full dataset. We show that uncertainty sampling based on least confidence score is competitive in traditional data collection but not applicable for overnight collection. We evaluate several active learning strategies for overnight data collection and show that different example selection strategies per domain perform best.
    语义分析的训练数据昂贵且收集缓慢。 我们将主动学习应用于传统和“隔夜”数据收集方法。 我们表明，可以从种子数据中获得良好的训练超参数，种子数据只是完整数据集的一小部分。 我们表明，基于最小置信度得分的不确定性抽样在传统数据收集中具有竞争力，但不适用于夜间收集。 我们评估了几种用于隔夜数据收集的主动学习策略，并显示每个域的不同示例选择策略表现最佳。


3.  GNEG: Graph-Based Negative Sampling for word2vec
    链接：https://aclanthology.coli.uni-saarland.de/papers/P18-2090/p18-2090
    Abstract: Negative sampling is an important component in word2vec for distributed word representation learning. We hypothesize that taking into account global, corpuslevel information and generating a different noise distribution for each target word better satisfies the requirements of negative examples for each training word than the original frequency-based distribution. In this purpose we pre-compute word cooccurrence statistics from the corpus and apply to it network algorithms such as random walk. We test this hypothesis through a set of experiments whose results show that our approach boosts the word analogy task by about 5% and improves the performance on word similarity tasks by about 1% compared to the skip-gram negative sampling baseline.